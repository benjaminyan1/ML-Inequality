{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36a0f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Variable       VIF\n",
      "0            latent_modernization  1.839760\n",
      "1                  latent_climate  2.452387\n",
      "2                 latent_earlydev  1.294030\n",
      "3                        euro2007  3.389650\n",
      "4  Populationdensitypeoplepers_ln  1.830087\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.905\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.903\n",
      "Method:                 Least Squares   F-statistic:                              366.0\n",
      "Date:                Tue, 10 Oct 2023   Prob (F-statistic):                    4.31e-96\n",
      "Time:                        19:44:14   Log-Likelihood:                         -776.31\n",
      "No. Observations:                 197   AIC:                                      1563.\n",
      "Df Residuals:                     192   BIC:                                      1579.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.7048      0.661     -4.093      0.000      -4.008      -1.401\n",
      "x2            -1.8854      0.742     -2.542      0.012      -3.348      -0.423\n",
      "x3            -5.3442      0.658     -8.118      0.000      -6.643      -4.046\n",
      "x4            -2.9095      3.124     -0.931      0.353      -9.071       3.252\n",
      "x5             8.6157      0.250     34.469      0.000       8.123       9.109\n",
      "==============================================================================\n",
      "Omnibus:                        7.600   Durbin-Watson:                   1.897\n",
      "Prob(Omnibus):                  0.022   Jarque-Bera (JB):               12.942\n",
      "Skew:                          -0.105   Prob(JB):                      0.00155\n",
      "Kurtosis:                       4.238   Cond. No.                         15.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Regression Equation: -2.7048*latent_modernization + -1.8854*latent_climate + -5.3442*latent_earlydev + -2.9095*euro2007 + 8.6157*Populationdensitypeoplepers_ln\n",
      "[-0.68775553  1.13249938 -1.34185956 -5.03079534 -0.26234941]\n",
      "41.71994839379245\n",
      "R-squared: 0.5378346292856095\n",
      "Mean Squared Error: 22.391078711080574\n",
      "R-squared: 0.4774515415545493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\benja\\OneDrive\\Documents\\Classes\\Machine Learning\\Project 2\\daniel_test.csv\")\n",
    "df = df.drop(columns = [ 'country', 'year'], axis = 1)\n",
    "df_filled = df.dropna()\n",
    "# Define the dependent variable and column names\n",
    "dependent_variable = 'gini_disp_ext'\n",
    "column_names = df.columns.tolist()\n",
    "# Remove the dependent variable from the list of column names\n",
    "column_names.remove(dependent_variable)\n",
    "important_columns = ['latent_modernization', 'latent_climate', 'latent_earlydev', 'euro2007',  'Populationdensitypeoplepers_ln', 'gini_disp_ext']\n",
    "X = ['latent_modernization', 'latent_climate', 'latent_earlydev', 'euro2007',  'Populationdensitypeoplepers_ln']\n",
    "Y = dependent_variable\n",
    "\n",
    "#get summary of data and save it to a csv file\n",
    "df[important_columns].describe().to_csv('SummaryRaw.csv', index = False)\n",
    "df_filled[important_columns].describe().to_csv('SummaryFiltered.csv', index= False)\n",
    "\n",
    "#print(df)\n",
    "#print(df_filled)\n",
    "\n",
    "# Get the point-biserial correlation of the dummy variables compared to the gini\n",
    "#correlation_coefficient, p_value = stats.pointbiserialr(df_filled['legor_so'], df_filled['gini_disp_ext'])\n",
    "#print(f\"Point-Biserial Correlation Coefficient: {correlation_coefficient}\")\n",
    "#print(f\"P-Value: {p_value}\")\n",
    "\n",
    "#Calculate the correlation matrix for all the independent variables, then narrow it down to X independent variables\n",
    "correlation_matrix = df.corr()\n",
    "correlation_X = df[important_columns].corr()\n",
    "#print(\"Correlation Matrix:\")\n",
    "#print(correlation_X)\n",
    "\n",
    "#Calculate the covariance matrix, then narrowed down to X independent variables\n",
    "numpy = df_filled.to_numpy()\n",
    "numpy_X = df_filled[important_columns].to_numpy()\n",
    "covariance_matrix = np.cov(numpy_X, rowvar=False)\n",
    "#print(\"Covariance Matrix:\")\n",
    "#print(covariance_matrix)\n",
    "\n",
    "\n",
    "#Calculate the Variance Inflation Matrix(VIF)\n",
    "important_dfilled = df_filled[X]\n",
    "variables = important_dfilled.values\n",
    "vif_values = [variance_inflation_factor(variables, i) for i in range(important_dfilled.shape[1])]\n",
    "# Create a DataFrame to display the results\n",
    "vif_df = pd.DataFrame({'Variable': important_dfilled.columns, 'VIF': vif_values})\n",
    "print(vif_df)\n",
    "\n",
    "# Create scatter plots for each column with respect to the dependent variable\n",
    "# for column in column_names:\n",
    "#    plt.figure(figsize=(8, 6))\n",
    "#    plt.scatter(df[column], df[dependent_variable], alpha=0.5)\n",
    "#    plt.title(f'Scatter Plot of {column} vs. {dependent_variable}')\n",
    "#    plt.xlabel(column)\n",
    "#    plt.ylabel(dependent_variable)\n",
    "#    plt.grid(True)\n",
    "#     plt.savefig(f'plot_{column}.png', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "#START OF REGRESSION CODE\n",
    "final_df = pd.read_csv(r\"C:\\Users\\benja\\OneDrive\\Documents\\Classes\\Machine Learning\\Project 2\\Final Filtered Data.csv\")\n",
    "x = final_df.loc[:, X].values\n",
    "y = final_df.loc[:, Y].values\n",
    "#normalize x\n",
    "x_norm = StandardScaler().fit_transform(x)\n",
    "\n",
    "#Create LINEAR REGRESSION MODEL FROM OLS\n",
    "#x = sm.add_constant(x)\n",
    "model = sm.OLS(y, x).fit()\n",
    "# Get the coefficients and variable names\n",
    "coefficientsOLS = model.params  # Include the intercept\n",
    "variable_names = X\n",
    "equation = \" + \".join(f\"{b:.4f}*{var}\" for b, var in zip(coefficientsOLS, variable_names))\n",
    "print(model.summary())\n",
    "print(f\"Regression Equation: {equation}\")\n",
    "\n",
    "#CREATE REGRESSION MODEL FROM SCIKIT LINEAR REGRESSION\n",
    "#Assuming X is your feature matrix (independent variables) and y is your target variable (dependent variable)\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# To get the coefficients (slopes) and intercept\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# To make predictions\n",
    "predictions = model.predict(x)\n",
    "print(coefficients)\n",
    "print(intercept)\n",
    "y_pred = model.predict(x)\n",
    "r_squared = r2_score(y, y_pred)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "#POLYNOMIAL REGRESSION\n",
    "#creates the training set and test set; test_size = 0.2 means 80% training 20% test; random_state is just an arbitrary number \n",
    "# that uses returns the same random values every time \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "X_train_poly = poly.fit_transform(x_train)\n",
    "X_test_poly = poly.transform(x_test)\n",
    "\n",
    "# Fit a linear regression model on the polynomial features\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
